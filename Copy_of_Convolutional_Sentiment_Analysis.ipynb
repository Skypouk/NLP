{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy_of_Convolutional_Sentiment_Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Skypouk/NLP/blob/main/Copy_of_Convolutional_Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eN6Iue50NWGo"
      },
      "source": [
        "Checking the existence of the tools and packages needed..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghKU0TFNTJXn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40f73141-eedc-41d2-86eb-2b08b65769f1"
      },
      "source": [
        "!pip3 install torch torchvision torchtext"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.7.0+cu101)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.8.1+cu101)\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.6/dist-packages (0.3.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.19.5)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch) (0.8)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (7.0.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext) (4.41.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2.10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zk1BknpyTwVk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75061e17-7ad8-491c-dcdf-a0a3e1ea15e3"
      },
      "source": [
        "! python3 -m spacy download en"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (53.0.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.4.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.4.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KLfNpGzxtOE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "568e0563-d848-440f-a7fa-54aa6e806a56"
      },
      "source": [
        "!pip install msgpack==0.5.6"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting msgpack==0.5.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/4e/dcf124fd97e5f5611123d6ad9f40ffd6eb979d1efdc1049e28a795672fcd/msgpack-0.5.6-cp36-cp36m-manylinux1_x86_64.whl (315kB)\n",
            "\r\u001b[K     |█                               | 10kB 22.4MB/s eta 0:00:01\r\u001b[K     |██                              | 20kB 27.8MB/s eta 0:00:01\r\u001b[K     |███▏                            | 30kB 29.2MB/s eta 0:00:01\r\u001b[K     |████▏                           | 40kB 32.5MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 51kB 33.9MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 61kB 35.6MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 71kB 36.2MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 81kB 23.5MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 92kB 21.9MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 102kB 23.3MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 112kB 23.3MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 122kB 23.3MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 133kB 23.3MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 143kB 23.3MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 153kB 23.3MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 163kB 23.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 174kB 23.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 184kB 23.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 194kB 23.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 204kB 23.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 215kB 23.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 225kB 23.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 235kB 23.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 245kB 23.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 256kB 23.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 266kB 23.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 276kB 23.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 286kB 23.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 296kB 23.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 307kB 23.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 317kB 23.3MB/s \n",
            "\u001b[?25hInstalling collected packages: msgpack\n",
            "  Found existing installation: msgpack 1.0.2\n",
            "    Uninstalling msgpack-1.0.2:\n",
            "      Successfully uninstalled msgpack-1.0.2\n",
            "Successfully installed msgpack-0.5.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJ1EbdHcLuWR"
      },
      "source": [
        "# 4 - Convolutional Sentiment Analysis\n",
        "\n",
        "In this notebook, we will be using a *convolutional neural network* (CNN) to conduct sentiment analysis.\n",
        "\n",
        "\n",
        "Traditionally, CNNs are used to analyse images and are made up of one or more *convolutional* layers, followed by one or more linear layers. The convolutional layers use filters (also called *kernels* or *receptive fields*) which scan across an image and produce a processed version of the image. This processed version of the image can be fed into another convolutional layer or a linear layer. Each filter has a shape, e.g. a 3x3 filter covers a 3 pixel wide and 3 pixel high area of the image, and each element of the filter has a weight associated with it, the 3x3 filter would have 9 weights. In traditional image processing these weights were specified by hand by engineers, however the main advantage of the convolutional layers is that these weights are learned via backpropagation. \n",
        "\n",
        "The intuitive idea behind learning the weights is that your convolutional layers act like *feature extractors*, extracting parts of the image that are most important for your CNN's goal, e.g. if using a CNN to detect faces in an image, the CNN may be looking for features such as the existance of a nose, mouth or a pair of eyes in the image.\n",
        "\n",
        "So why use CNNs on text? In the same way that a 3x3 filter can look over a patch of an image, a 1x2 filter can look over a 2 sequential words in a piece of text, i.e. a bi-gram. In the previous tutorial we looked at the FastText model which used bi-grams by explicitly adding them to the end of a text, in this CNN model we will instead use multiple filters of different sizes which will look at the bi-grams (a 1x2 filter), tri-grams (a 1x3 filter) and n-grams (a 1x$n$ filter) within the text.\n",
        "\n",
        "The intuition here is that the appearance of certain bi-grams, tri-grams and n-grams within the review will be a good indication of the final sentiment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xel7fyT3LuWS"
      },
      "source": [
        "## Preparing Data\n",
        "\n",
        "Let's prepare the data. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6p4e0_M3LuWT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edf81d16-f2c7-4df5-b6b0-47542ac5e1ae"
      },
      "source": [
        "import torch\n",
        "from torchtext import data\n",
        "from torchtext import datasets\n",
        "import random\n",
        "\n",
        "SEED = 1234\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "\n",
        "TEXT = data.Field(tokenize='spacy')\n",
        "LABEL = data.LabelField()\n",
        "\n",
        "train, test = datasets.IMDB.splits(TEXT, LABEL)\n",
        "\n",
        "train, valid = train.split(random_state=random.seed(SEED))\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\raclImdb_v1.tar.gz:   0%|          | 0.00/84.1M [00:00<?, ?B/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading aclImdb_v1.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:03<00:00, 22.9MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "miIxzbHaXLEy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fec11ad-84c9-4cf9-c6de-e7f8d3e0de38"
      },
      "source": [
        "len(train[0].text)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "91"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfKsp49rLuWY"
      },
      "source": [
        "Build the vocab and load the pre-trained word embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVoAfhRPLuWZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7edd0cdf-9013-4ba7-f160-83a0ebc5c9f2"
      },
      "source": [
        "TEXT.build_vocab(train, max_size=25000, vectors=\"glove.6B.100d\")\n",
        "LABEL.build_vocab(train)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [07:32, 1.91MB/s]                          \n",
            "100%|█████████▉| 399154/400000 [00:16<00:00, 23594.57it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w518rTPFw7f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76bea19d-86dd-419b-fbfc-a624005cc297"
      },
      "source": [
        "test_w=TEXT.vocab.itos[9205]\n",
        "test_w2=TEXT.vocab.itos[9206]\n",
        "\n",
        "print(test_w)\n",
        "print(test_w2)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "encouraged\n",
            "enduring\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MD4vi9in74Qo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1aeeee48-96d6-4ead-970f-c1770bc45b29"
      },
      "source": [
        "from torch.nn.functional import cosine_similarity\n",
        "test_v=TEXT.vocab.vectors[9205].unsqueeze(0)\n",
        "test_v2=TEXT.vocab.vectors[9206]\n",
        "\n",
        "cosine_similarity(test_v,TEXT.vocab.vectors,dim=1).sort()\n",
        "\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.return_types.sort(values=tensor([-0.4001, -0.3987, -0.3701,  ...,  0.7593,  0.7693,  1.0000]), indices=tensor([20166, 14299,  5204,  ...,  5456, 11636,  9205]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDkI8M1dvOrH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "481b35ea-c609-4d13-e20e-417caeb861dd"
      },
      "source": [
        "print(TEXT.vocab.itos[538])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "amazing\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZciSLaeALuWb"
      },
      "source": [
        "As before, we create the iterators."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzlpJr3eLuWc"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train, valid, test), \n",
        "    batch_size=BATCH_SIZE, \n",
        "    sort_key=lambda x: len(x.text), \n",
        "    repeat=False,\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu'))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPFkflGKLuWg"
      },
      "source": [
        "## Build the Model\n",
        "\n",
        "Now to build our model.\n",
        "\n",
        "The first major hurdle is visualizing how CNNs are used for text. Images are typically 2 dimensional (we'll ignore the fact that there is a third \"colour\" dimension for now) whereas text is 1 dimensional. However, we know that the first step is converting the words into vectors. This is how we can visualize our words in 2 dimensions, each word along one axis and the elements of vectors aross the other dimension. Consider the 2 dimensional representation of the embedded sentence below:\n",
        "\n",
        "![](https://i.imgur.com/ci1h9hv.png)\n",
        "\n",
        "We can then use a filter that is **[n x emb_dim]**. This will cover $n$ sequential words entirely, as their width will be `emb_dim` dimensions. Consider the image below. Our word vectors are represented in green, here we have 4 words with 5 dimensional embeddings, creating a 4x5 \"image\". A filter that covers two words at a time (i.e. bi-grams) will be **[2x5]**, shown in yellow. The output of this filter (shown in red) will be a single real number that is the weighted sum of all elements covered by the filter.\n",
        "\n",
        "![](https://i.imgur.com/QlXduXu.png)\n",
        "\n",
        "The filter then moves \"down\" the image (or across the sentence) to cover the next bi-gram and another output is calculated. \n",
        "\n",
        "![](https://i.imgur.com/wuA330x.png)\n",
        "\n",
        "Finally, the filter moves down again and the final output for this filter is calculated.\n",
        "\n",
        "![](https://i.imgur.com/gi1GaEz.png)\n",
        "\n",
        "\n",
        "In our model, we will also have different sizes of filters, heights of 3, 4 and 5, with 100 of each of them. The intuition is that we will be looking for the occurence of different tri-grams, 4-grams and 5-grams that are relevant for analysing sentiment of movie reviews.\n",
        "\n",
        "The next step in our model is to use *pooling* (specifically *max pooling*) on the output of the convolutional layers. We are taking the maximum value over a dimension. Below an example of taking the maximum value (0.9) from the output of the convolutional layer on the example sentence (not shown in the activation function applied to the output of the convolutions).\n",
        "\n",
        "![](https://i.imgur.com/gzkS3ze.png)\n",
        "\n",
        "The idea here is that the maximum value is the \"most important\" feature for determining the sentiment of the review, this corresponds to the \"most important\" n-gram within the review. How do we know what the \"most important\" n-gram is? Luckily, we don't have to! Through backpropagation, the weights of the filters are changed so that whenever certain n-grams that correspond to a sentiment are seen, the output of the filter is a \"high\" value. This \"high\" value then passes through the max pooling layer if it is the maximum value in the output. \n",
        "\n",
        "As our model has 100 filters of 3 different sizes, that means we have 300 different n-grams the model thinks are important. We concatenate these together into a single vector and pass them through a linear layer to predict the sentiment. We can think of the weights of this linear layer as \"weighting up the evidence\" from each of the 300 n-grams and making a final decision. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtfYscMdLuWh"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, dropout):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.conv_0 = nn.Conv2d(in_channels=1, out_channels=n_filters, kernel_size=(filter_sizes[0],embedding_dim))\n",
        "        self.conv_1 = nn.Conv2d(in_channels=1, out_channels=n_filters, kernel_size=(filter_sizes[1],embedding_dim))\n",
        "        self.conv_2 = nn.Conv2d(in_channels=1, out_channels=n_filters, kernel_size=(filter_sizes[2],embedding_dim))\n",
        "        self.fc = nn.Linear(len(filter_sizes)*n_filters, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        #x = [sent len, batch size]\n",
        "        \n",
        "        x = x.permute(1, 0)\n",
        "                \n",
        "        #x = [batch size, sent len]\n",
        "        \n",
        "        embedded = self.embedding(x)\n",
        "                \n",
        "        #embedded = [batch size, sent len, emb dim]\n",
        "        \n",
        "        embedded = embedded.unsqueeze(1)\n",
        "        \n",
        "        #embedded = [batch size, 1, sent len, emb dim]\n",
        "        \n",
        "        conved_0 = F.relu(self.conv_0(embedded).squeeze(3))\n",
        "        conved_1 = F.relu(self.conv_1(embedded).squeeze(3))\n",
        "        conved_2 = F.relu(self.conv_2(embedded).squeeze(3))\n",
        "            \n",
        "        #conv_n = [batch size, n_filters, sent len - filter_sizes[n]]\n",
        "        \n",
        "        pooled_0 = F.max_pool1d(conved_0, conved_0.shape[2]).squeeze(2)\n",
        "        pooled_1 = F.max_pool1d(conved_1, conved_1.shape[2]).squeeze(2)\n",
        "        pooled_2 = F.max_pool1d(conved_2, conved_2.shape[2]).squeeze(2)\n",
        "        \n",
        "        #pooled_n = [batch size, n_filters]\n",
        "        \n",
        "        cat = self.dropout(torch.cat((pooled_0, pooled_1, pooled_2), dim=1))\n",
        "\n",
        "        #cat = [batch size, n_filters * len(filter_sizes)]\n",
        "            \n",
        "        return self.fc(cat)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJqPRQpJLuWp"
      },
      "source": [
        "We create an instance of our `CNN` class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1slU0EyXLuWr"
      },
      "source": [
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 100\n",
        "N_FILTERS = 100\n",
        "FILTER_SIZES = [3,4,5]\n",
        "OUTPUT_DIM = 1\n",
        "DROPOUT = 0.5\n",
        "\n",
        "model = CNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Q4lJLY_LuWu"
      },
      "source": [
        "And load the pre-trained embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eC0EJAwLLuWu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9e6c18b-1444-4c15-b1ce-dbc8d0f062ab"
      },
      "source": [
        "pretrained_embeddings = TEXT.vocab.vectors\n",
        "\n",
        "model.embedding.weight.data.copy_(pretrained_embeddings)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
              "        ...,\n",
              "        [ 0.2879, -0.2096,  0.2155,  ..., -0.0165, -0.3002,  0.5226],\n",
              "        [-0.4165,  0.0542, -0.1853,  ..., -0.2558, -0.2313,  1.0969],\n",
              "        [ 0.4557,  0.5966,  0.0917,  ..., -0.6968, -0.2632, -0.4583]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNDsXPRJLuW1"
      },
      "source": [
        "## Train the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lf7N4ewnLuW1"
      },
      "source": [
        "Training is the same as before. We initialize the optimizer, loss function (criterion) and place the model and criterion on the GPU (if available)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVADtymyLuW3"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NH8WT9CSLuW6"
      },
      "source": [
        "We implement the function to calculate accuracy..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "az-c9zfwLuW7"
      },
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def binary_accuracy(preds, y):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
        "    \"\"\"\n",
        "\n",
        "    #round predictions to the closest integer\n",
        "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
        "    correct = (rounded_preds == y.float()).float() #convert into float for division \n",
        "    acc = correct.sum()/len(correct)\n",
        "    return acc"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18CGWz6XLuW-"
      },
      "source": [
        "We define a function for training our model...\n",
        "\n",
        "**Note**: as we are using dropout again, we must remember to use `model.train()` to ensure the dropout is \"turned on\" while training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggAOZpM6LuW_"
      },
      "source": [
        "def train_model(model, iterator, optimizer, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    for batch in iterator:\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        predictions = model(batch.text).squeeze(1)\n",
        "        \n",
        "        loss = criterion(predictions, batch.label.float())\n",
        "        \n",
        "        acc = binary_accuracy(predictions, batch.label)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STUa1L7PLuXC"
      },
      "source": [
        "We define a function for testing our model...\n",
        "\n",
        "**Note**: again, as we are now using dropout, we must remember to use `model.eval()` to ensure the dropout is \"turned off\" while evaluating."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXd9-JLsLuXE"
      },
      "source": [
        "def evaluate_model(model, iterator, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch in iterator:\n",
        "\n",
        "            predictions = model(batch.text).squeeze(1)\n",
        "            \n",
        "            loss = criterion(predictions, batch.label.float())\n",
        "            \n",
        "            acc = binary_accuracy(predictions, batch.label)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XOzP0WVLuXI"
      },
      "source": [
        "Finally, we train our model..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "6-pbGKqhLuXI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40d896f1-2841-4579-cd75-db6e54557c15"
      },
      "source": [
        "  N_EPOCHS = 5\n",
        "  for epoch in range(N_EPOCHS):\n",
        "\n",
        "      train_loss, train_acc = train_model(model, train_iterator, optimizer, criterion)\n",
        "      #valid_loss, valid_acc = evaluate_model(model, valid_iterator, criterion)\n",
        "      valid_loss=0\n",
        "      valid_acc=0\n",
        "\n",
        "      print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc*100:.2f}%, Val. Loss: {valid_loss:.3f}, Val. Acc: {valid_acc*100:.2f}%')\n",
        "\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r100%|█████████▉| 399154/400000 [00:30<00:00, 23594.57it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01, Train Loss: 0.502, Train Acc: 73.99%, Val. Loss: 0.000, Val. Acc: 0.00%\n",
            "Epoch: 02, Train Loss: 0.310, Train Acc: 86.93%, Val. Loss: 0.000, Val. Acc: 0.00%\n",
            "Epoch: 03, Train Loss: 0.224, Train Acc: 91.25%, Val. Loss: 0.000, Val. Acc: 0.00%\n",
            "Epoch: 04, Train Loss: 0.152, Train Acc: 94.36%, Val. Loss: 0.000, Val. Acc: 0.00%\n",
            "Epoch: 05, Train Loss: 0.093, Train Acc: 96.95%, Val. Loss: 0.000, Val. Acc: 0.00%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNnkGoitLuXM"
      },
      "source": [
        "...and get our best test accuracy yet! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmLlA56oLuXN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbf853b0-defd-4353-9d62-1c524aed5eb2"
      },
      "source": [
        "test_loss, test_acc = evaluate_model(model, test_iterator, criterion)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f}, Test Acc: {test_acc*100:.2f}%')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 0.304, Test Acc: 88.34%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbedQ2sZLuXS"
      },
      "source": [
        "import spacy\n",
        "import numpy as np\n",
        "from IPython.core.display import display,HTML\n",
        "from torch.nn.functional import cosine_similarity\n",
        "\n",
        "nlp = spacy.load('en')\n",
        "eps = np.finfo(np.float32).eps.item()\n",
        "def predict_sentiment(sentence, explain_scores=True,explain_relative_to=1):\n",
        "  \n",
        "    tokenized_sentence = [tok.text for tok in nlp.tokenizer(sentence)]\n",
        "    indexed_sentence = [TEXT.vocab.stoi[t] for t in tokenized_sentence]\n",
        "    tensor = torch.LongTensor(indexed_sentence).to(device)\n",
        "    tensor = tensor.unsqueeze(1)\n",
        "    prediction = torch.sigmoid(model(tensor))\n",
        "    \n",
        "    original_input_embedding,input_grad=get_input_gradients(indexed_sentence,prediction,explain_relative_to)\n",
        "    \n",
        "    explanation=get_prediction_explanation(tokenized_sentence,original_input_embedding,input_grad,explain_scores)\n",
        "      \n",
        "    return {'tokenized_sentence': tokenized_sentence,'prediction': prediction.item(),'explanation': explanation }\n",
        "\n",
        "def get_input_gradients(original_sentence,prediction,in_relation_to):\n",
        "    gradient_truth=torch.Tensor([in_relation_to]).unsqueeze(0)\n",
        "    if torch.cuda.is_available():\n",
        "      gradient_truth=gradient_truth.cuda()\n",
        "    \n",
        "    loss=criterion(prediction,gradient_truth)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    \n",
        "    input_grad=torch.Tensor(len(original_sentence),model.embedding.weight.size(1))\n",
        "    original_input_embedding=torch.Tensor(len(original_sentence),model.embedding.weight.size(1))\n",
        "    \n",
        "    for i in range(0,len(original_sentence)):\n",
        "      original_input_embedding[i]=model.embedding.weight[original_sentence[i]]\n",
        "      input_grad[i]=model.embedding.weight.grad[original_sentence[i]]\n",
        "    \n",
        "    return original_input_embedding,input_grad\n",
        "    \n",
        "    \n",
        "\n",
        "def get_input_scores(input,input_embedding,input_grad):\n",
        "  \n",
        "  \n",
        "  # Take a SGD step using grads\n",
        "  \n",
        "  input_after_step=input_embedding-input_grad\n",
        "  after_grad_norms = torch.norm(input_after_step, 2, 1)\n",
        "  before_grad_norms = torch.norm(input_embedding, 2, 1)\n",
        "  variation = after_grad_norms-before_grad_norms\n",
        " \n",
        "  standard_deviation=torch.std(variation)\n",
        "  mean=torch.mean(variation)\n",
        "  z_score=(variation-mean)/standard_deviation\n",
        " \n",
        "  return z_score\n",
        "\n",
        "def old_get_input_scores(input,input_embedding,input_grad):\n",
        "  \n",
        "  grad_norms=torch.norm(input_grad,2,1)\n",
        "  \n",
        "  return grad_norms/torch.max(grad_norms)\n",
        "\n",
        "  \n",
        "def get_prediction_explanation(input,input_embedding,input_grad, explain_scores):\n",
        " \n",
        "  \n",
        "  input_word_scores=get_input_scores(input,input_embedding,input_grad)\n",
        "   \n",
        "  explanation=\"\"\n",
        "  for i in range(0,len(input)):\n",
        "    token=input[i]\n",
        "    if explain_scores:\n",
        "      str_token=\"%s (%.3f)\"%(token,input_word_scores[i])\n",
        "    else:\n",
        "      str_token=token\n",
        "    \n",
        "    explanation=explanation+str_token+'&nbsp;</font>'\n",
        "    if i>0 and i%20==0:\n",
        "      explanation=explanation+\"<br/>\"\n",
        "    \n",
        "  return {'word_scores': input_word_scores,'input_gradient': input_grad,'textual_explanation':explanation }\n",
        "\n",
        "  \n",
        "  "
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VueXNHCzHC9w"
      },
      "source": [
        "from torch.nn.functional import cosine_similarity\n",
        "\n",
        "def get_projected_words(word,word_gradient,num_words=1):\n",
        "  \n",
        "  word_index=TEXT.vocab.stoi[word]\n",
        "  word_embedding=TEXT.vocab.vectors[word_index]\n",
        "  learning_rate=1\n",
        "  i=0\n",
        "  result=[]\n",
        "  \n",
        "  while i<100000:\n",
        "    try: \n",
        "      word_embedding=word_embedding-learning_rate*word_gradient\n",
        "    except:\n",
        "      # We can have a float overflow here if this process gets out of control\n",
        "      return result\n",
        "    similarity_value,similarity_index=cosine_similarity(word_embedding.unsqueeze(0),TEXT.vocab.vectors,dim=1).sort(descending=True)\n",
        "    if similarity_index[0]!=word_index:\n",
        "      if  similarity_value[0]<0.5:\n",
        "        break\n",
        "      \n",
        "      result.append({'word':TEXT.vocab.itos[similarity_index[0]],'similarity': similarity_value[0]})\n",
        "      word_index=similarity_index[0]\n",
        "      learning_rate=1\n",
        "      if len(result)>=num_words:\n",
        "        break\n",
        "      \n",
        "    i=i+1\n",
        "    learning_rate=learning_rate*1.1\n",
        "      \n",
        "  return result\n",
        "\n",
        "  \n",
        "def get_projected_sentence_word(prediction,word):\n",
        "  \n",
        "  sentence=prediction['tokenized_sentence']\n",
        "  word_index_in_sentence=[i for i in range(0,len(sentence)) if sentence[i]==word][0]\n",
        "  word_gradient=prediction['explanation']['input_gradient'][word_index_in_sentence]\n",
        "  \n",
        "  \n",
        "  return get_projected_words(word,word_gradient,1)\n",
        "  \n",
        "  "
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIm9_KJRLuXb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "a4e04ff9-3811-4ad1-b8fb-b9b1da29f02c"
      },
      "source": [
        "\n",
        "\n",
        "prediction=predict_sentiment(\"This is a ridiculous movie and you should never see it.\")\n",
        "print(prediction['prediction'])\n",
        "display(HTML(prediction['explanation']['textual_explanation']))\n",
        "print(get_projected_sentence_word(prediction,'ridiculous'))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.195659339427948\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "This (0.943)&nbsp;</font>is (0.542)&nbsp;</font>a (0.454)&nbsp;</font>ridiculous (-2.971)&nbsp;</font>movie (-0.235)&nbsp;</font>and (0.803)&nbsp;</font>you (0.274)&nbsp;</font>should (0.049)&nbsp;</font>never (0.031)&nbsp;</font>see (0.030)&nbsp;</font>it (0.054)&nbsp;</font>. (0.025)&nbsp;</font>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "[{'word': 'amazing', 'similarity': tensor(0.6718)}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sC7nv-Xe3Xne",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "163a57b1-3c9f-49bb-818e-97d3cd191e2b"
      },
      "source": [
        "\n",
        "prediction=predict_sentiment(\"By Timandra Harkness It is a glorious film, but you could not make it now. And that is not just my opinion. \")\n",
        "print(prediction['prediction'])\n",
        "display(HTML(prediction['explanation']['textual_explanation']))\n",
        "\n",
        "\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9711049199104309\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "By (-0.116)&nbsp;</font>Timandra (-0.624)&nbsp;</font>Harkness (-0.624)&nbsp;</font>It (-0.889)&nbsp;</font>is (0.570)&nbsp;</font>a (-0.581)&nbsp;</font>glorious (4.519)&nbsp;</font>film (0.419)&nbsp;</font>, (-0.457)&nbsp;</font>but (-0.335)&nbsp;</font>you (-0.174)&nbsp;</font>could (-0.248)&nbsp;</font>not (-0.283)&nbsp;</font>make (-0.115)&nbsp;</font>it (-0.161)&nbsp;</font>now (-0.161)&nbsp;</font>. (-0.188)&nbsp;</font>And (-0.231)&nbsp;</font>that (-0.170)&nbsp;</font>is (0.570)&nbsp;</font>not (-0.283)&nbsp;</font><br/>just (-0.193)&nbsp;</font>my (-0.043)&nbsp;</font>opinion (-0.013)&nbsp;</font>. (-0.188)&nbsp;</font>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CF20HeKoJU2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "985b56f1-3551-41a3-a90e-da5376a8b321"
      },
      "source": [
        "print(get_projected_sentence_word(prediction,'glorious'))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[{'word': 'splendid', 'similarity': tensor(0.5050)}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIym-iUnqAOC"
      },
      "source": [
        "def display_message(message):\n",
        "  display(HTML(message))\n",
        "  \n",
        "def predict_and_make_it_better(text,better_direction=1):\n",
        "  \n",
        "  version=0\n",
        "  word_to_change=None\n",
        "  better_word=None\n",
        "  \n",
        "  while True:\n",
        "    \n",
        "    prediction=predict_sentiment(text,explain_scores=False,explain_relative_to=better_direction)\n",
        "    display_message(\"<H2> Version \"+str(version)+\"</H2>\")\n",
        "    if word_to_change!=None:\n",
        "      display_message(\"<H3>\"+word_to_change+\"->\"+better_word+\"</H3>\")\n",
        "      \n",
        "    display_message(\"<H3> Sentiment: \"+str(prediction['prediction'])+\"+</H3>\")\n",
        "    display(HTML(prediction['explanation']['textual_explanation']))\n",
        "    \n",
        "    # Get the word with the highest absolute score\n",
        "    word_to_change=None\n",
        "    better_word=None\n",
        "  \n",
        "    word_scores=prediction['explanation']['word_scores']\n",
        "    _,sorted_indices=torch.abs(word_scores).sort(descending=True)\n",
        "    changed_text=False\n",
        "    for i in range(0,sorted_indices.size(0)):\n",
        "      tokenized_sentence=prediction['tokenized_sentence']\n",
        "      word_to_change=tokenized_sentence[sorted_indices[i]]\n",
        "      better_words=get_projected_sentence_word(prediction,word_to_change)\n",
        "      if len(better_words)>0:\n",
        "        better_word=better_words[0]['word']\n",
        "        new_tokenized_sentence=[t if t!=word_to_change else better_word for t in tokenized_sentence]\n",
        "        text=\" \".join(new_tokenized_sentence)\n",
        "        changed_text=True\n",
        "        break\n",
        "    \n",
        "    if not changed_text:\n",
        "      return\n",
        "    \n",
        "    version=version+1\n",
        "    \n",
        "               \n",
        "    \n",
        "    "
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrDvD0_Sq_Pn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4cde9108-9c25-4ffe-db26-5a76dda556b4"
      },
      "source": [
        "predict_and_make_it_better(\"bad worse stupid not funny\",better_direction=1)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<H2> Version 0</H2>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<H3> Sentiment: 0.08372706919908524+</H3>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "bad&nbsp;</font>worse&nbsp;</font>stupid&nbsp;</font>not&nbsp;</font>funny&nbsp;</font>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<H2> Version 1</H2>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<H3>worse->even</H3>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<H3> Sentiment: 0.2438199818134308+</H3>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "bad&nbsp;</font>even&nbsp;</font>stupid&nbsp;</font>not&nbsp;</font>funny&nbsp;</font>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<H2> Version 2</H2>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<H3>stupid->crazy</H3>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<H3> Sentiment: 0.4628046452999115+</H3>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "bad&nbsp;</font>even&nbsp;</font>crazy&nbsp;</font>not&nbsp;</font>funny&nbsp;</font>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<H2> Version 3</H2>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<H3>bad->good</H3>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<H3> Sentiment: 0.5191659927368164+</H3>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "good&nbsp;</font>even&nbsp;</font>crazy&nbsp;</font>not&nbsp;</font>funny&nbsp;</font>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<H2> Version 4</H2>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<H3>good->excellent</H3>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<H3> Sentiment: 0.5564008355140686+</H3>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "excellent&nbsp;</font>even&nbsp;</font>crazy&nbsp;</font>not&nbsp;</font>funny&nbsp;</font>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<H2> Version 5</H2>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<H3>not->yet</H3>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<H3> Sentiment: 0.6554448008537292+</H3>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "excellent&nbsp;</font>even&nbsp;</font>crazy&nbsp;</font>yet&nbsp;</font>funny&nbsp;</font>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<H2> Version 6</H2>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<H3>crazy->amazing</H3>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<H3> Sentiment: 0.9320388436317444+</H3>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "excellent&nbsp;</font>even&nbsp;</font>amazing&nbsp;</font>yet&nbsp;</font>funny&nbsp;</font>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<H2> Version 7</H2>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<H3>even->well</H3>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<H3> Sentiment: 0.9981619715690613+</H3>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "excellent&nbsp;</font>well&nbsp;</font>amazing&nbsp;</font>yet&nbsp;</font>funny&nbsp;</font>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<H2> Version 8</H2>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<H3>yet->remarkable</H3>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<H3> Sentiment: 0.9998362064361572+</H3>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "excellent&nbsp;</font>well&nbsp;</font>amazing&nbsp;</font>remarkable&nbsp;</font>funny&nbsp;</font>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<H2> Version 9</H2>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<H3>funny->captivating</H3>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<H3> Sentiment: 0.9999805688858032+</H3>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "excellent&nbsp;</font>well&nbsp;</font>amazing&nbsp;</font>remarkable&nbsp;</font>captivating&nbsp;</font>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<H2> Version 10</H2>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<H3>well->excellent</H3>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<H3> Sentiment: 0.9999974966049194+</H3>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "excellent&nbsp;</font>excellent&nbsp;</font>amazing&nbsp;</font>remarkable&nbsp;</font>captivating&nbsp;</font>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}